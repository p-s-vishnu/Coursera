{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = tf.constant([5, 3, 8], dtype=tf.float32, name=\"variable_a\")\n",
    "x = tf.Variable(2.0, dtype=tf.float32, name=\"my_variable\")\n",
    "# x.assign(new_value)\n",
    "# x.assign_add(value_to_be_added)\n",
    "# x.assign_sub(value_to_be_subtracted\n",
    "\n",
    "\n",
    "# Point operation\n",
    "# tf.add allows to add the components of a tensor\n",
    "# tf.multiply allows us to multiply the components of a tensor\n",
    "# tf.subtract allow us to substract the components of a tensor\n",
    "# tf.math.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf \n",
    "\n",
    "def loss_mse(X,Y,w0,w1):\n",
    "    Y_hat = X*w0 + w1\n",
    "    return tf.reduce_mean(tf.square(Y_hat - Y))  # alternate: tf.sub\n",
    "\n",
    "def compute_gradient(X, Y, w0, w1):\n",
    "    with tf.GradientTape as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])\n",
    "\n",
    "# TRAINING    \n",
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.02\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "for step in range(0, STEPS + 1):\n",
    "    dw0, dw1 = compute_gradients(X,Y,w0,w1)\n",
    "    w0.assign_sub(dw0*LEARNING_RATE) # GRADIENT -> DIRECTION, LR -> THE AMOUNT OF MOVEMENT\n",
    "    w1.assign_sub(dw1*LEARNING_RATE) \n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X,Y,w0,w1)  # TODO: Your code goes here.\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    f1 = tf.ones_like(X)  # Bias.\n",
    "    f2 = X\n",
    "    f3 = tf.square(X)\n",
    "    f4 = tf.sqrt(X)\n",
    "    f5 = tf.exp(X)\n",
    "    return tf.stack([f1, f2, f3, f4, f5], axis=1)\n",
    "\n",
    "def predict(X, W):\n",
    "    return tf.squeeze(X @ W, -1)\n",
    "\n",
    "X = tf.constant(np.linspace(0, 2, 1000), dtype=tf.float32)\n",
    "Y = X * tf.exp(-(X ** 2))\n",
    "\n",
    "plt.plot(X, Y)\n",
    "\n",
    "STEPS = 2000\n",
    "LEARNING_RATE = 0.02\n",
    "Xf = make_features(X)\n",
    "n_weights = Xf.shape[1]\n",
    "W = tf.Variable(np.zeros((n_weights, 1)), dtype=tf.float32)\n",
    "\n",
    "# PLOTTING\n",
    "steps, losses = [], []\n",
    "plt.figure()\n",
    "for step in range(1, STEPS + 1):\n",
    "    dW = compute_gradients(X, Y, W)\n",
    "    W.assign_sub(dW * LEARNING_RATE)\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(Xf, Y, W)\n",
    "        steps.append(step)\n",
    "        losses.append(loss)\n",
    "        plt.clf()\n",
    "        plt.plot(steps, losses)\n",
    "print(f\"STEP: {STEPS} MSE: {loss_mse(Xf, Y, W)}\")\n",
    "plt.figure()\n",
    "plt.plot(X, Y, label=\"actual\")\n",
    "plt.plot(X, predict(Xf, W), label=\"predicted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sequential API\n",
    "- feature columns for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "print(tf.__version__)\n",
    "%matplotlib inline\n",
    "\n",
    "# taxi dataset\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    for unwanted_col in UNWANTED_COLS: features.pop(unwanted_col)\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS      # \n",
    "    )\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    if mode == \"train\": dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_datetime\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "    \"key\",\n",
    "]\n",
    "LABEL_COLUMN = \"fare_amount\"\n",
    "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]   # COLUMNS DEFAULTS\n",
    "UNWANTED_COLS = [\"pickup_datetime\", \"key\"]\n",
    "\n",
    "INPUT_COLS = [\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "\n",
    "feature_columns =  [\n",
    "    tf.feature_column.numeric_column(key=key)\n",
    "    for key in INPUT_COLS\n",
    "]\n",
    "# Build a keras DNN model using Sequential API\n",
    "model =  keras.Sequential([\n",
    "    DenseFeatures(feature_columns=feature_columns),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"relu\", name=\"h1\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation=\"relu\", name=\"h2\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation=\"relu\", name=\"h3\"),\n",
    "    Dense(1, activation=\"linear\", name=\"output\")\n",
    "])\n",
    "\n",
    "# COMPILE\n",
    "def rmse(y_true, y_pred):\n",
    "    return  tf.reduce_mean(tf.square(tf.subtract(y_pred, y_true)))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "# TRAINING\n",
    "# .fit() for training a model for a fixed number of epochs (iterations on a dataset).\n",
    "# .fit_generat\n",
    "TRAIN_BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 50  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern=\"../data/taxi-train*\", batch_size=TRAIN_BATCH_SIZE, mode=\"train\"\n",
    ")\n",
    "evalds = create_dataset(\n",
    "    pattern=\"../data/taxi-valid*\", batch_size=1000, mode=\"eval\"\n",
    ").take(NUM_EVAL_EXAMPLES // 1000)or() for training a model on data yielded batch-by-batch by a generator\n",
    "# .train_on_batch() runs a single gradient update on a single batch of data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
